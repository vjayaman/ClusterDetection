---
title: "Focus on T2"
output: flexdashboard::flex_dashboard
---

```{r setup, include=FALSE}
source("global.R")
knitr::opts_chunk$set(echo = FALSE)
```

Column 
-------------------------------------
    
### Investigation of the data
    
Reading in the data  

```{r}
campy_data <- c("MergedFinal_332calls_4203genomes_clusters.tsv", 
                "MergedFinal_332calls_5075genomes_clusters.tsv")

X <- 1
Y <- 2

timepoint1 <- read.csv(file = campy_data[1], numerals = "no.loss",
                       stringsAsFactors = FALSE, sep = "\t") %>% as_tibble()
colnames(timepoint1) %<>% gsub(pattern = "genome", replacement = "isolate", x = .) %>%
  gsub(pattern = "X", replacement = "h_", x = .)

timepoint2 <- read.csv(file = campy_data[2], numerals = "no.loss", 
                       stringsAsFactors = FALSE, sep = "\t") %>% as_tibble()
colnames(timepoint2) %<>% gsub(pattern = "genome", replacement = "isolate", x = .) %>% 
  gsub(pattern = "X", replacement = "h_", x = .)
```
   
These are the isolates with designated clusters and corresponding cluster sizes at time point ___, for all heights.
```{r}
# timepoint X
sizes_for_tpX <- clusterSizes(timepoint1)
tpX <- isolateClusterSize(timepoint1, sizes_for_tpX) %>%
  set_colnames(c("isolate", "heights", "tpX_clusters", "tpX_sizes"))

# timepoint Y
sizes_for_tpY <- clusterSizes(timepoint2)
tpY <- isolateClusterSize(timepoint2, sizes_for_tpY) %>%
  set_colnames(c("isolate", "heights", "tpY_clusters", "tpY_sizes"))
```

Some isolates occur in only one of the two time point datasets, so the following is to keep track of this:
# Tracking isolates
```{r}
# setting up dataframe
d <- c(timepoint1$isolate, timepoint2$isolate) %>% unique() %>%
  tibble(Isolates = ., TPX = FALSE, TPY = FALSE, Both = FALSE)

# identifying isolates in one or both
d$TPY[which(d$Isolates %in% timepoint2$isolate)] <- TRUE
d$TPX[which(d$Isolates %in% timepoint1$isolate)] <- TRUE
d$Both[which(d$TPX & d$TPY)] <- TRUE

# preparing dataset for display
d <- d[order(d$Isolates),]
vals <- c(TRUE, FALSE)
```

```{r}
# table with colorful cells indicating where isolates are found
d %>% set_colnames(c("Isolates", paste0("Found in time point ", X, " dataset"),
                     paste0("Found in time point ", Y, " dataset"), "Found in both datasets")) %>%
  DT::datatable(., fillContainer = TRUE, filter = "top", rownames = TRUE,
                options = list(pageLength = nrow(d), dom = "ti", scrollY = "500px")) %>%
  formatStyle(2, backgroundColor = styleEqual(vals, c("lightsteelblue", "gray"))) %>%
  formatStyle(3, backgroundColor = styleEqual(vals, c("lightgreen", "gray"))) %>%
  formatStyle(4, backgroundColor = styleEqual(vals, c("lightsteelblue", "gray")))
```

Filtered to show only novel genomes

```{r}
d[which(d$TPX == FALSE),] %>% 
  set_colnames(c("Isolates", paste0("Found in time point ", X, " dataset"),
                     paste0("Found in time point ", Y, " dataset"), "Found in both datasets")) %>%
  DT::datatable(., fillContainer = TRUE, filter = "top", rownames = TRUE,
                options = list(pageLength = nrow(d), dom = "ti", scrollY = "500px")) %>%
  formatStyle(2, backgroundColor = styleEqual(vals, c("lightsteelblue", "gray"))) %>%
  formatStyle(3, backgroundColor = styleEqual(vals, c("lightgreen", "gray"))) %>%
  formatStyle(4, backgroundColor = styleEqual(vals, c("lightsteelblue", "gray")))
```

```{r}
# isolates at time point 2 and not time point 1
isolates_t2 <- d$Isolates[d$TPY & !d$TPX]
# isolates at time point 1
isolates_t1 <- d$Isolates[d$TPX]
```

Want to find the first threshold where a new genome (added at time point 2) is part of a multistrain cluster that includes genomes from time point 1.  

  1. isolate_k is the k-th isolate of those that are introduced at time point 2  
  
  2. we filter the time point 2 dataset to only contain isolate_k and all height-cluster pairs where isolate_k is found in a cluster of size > 1  
  
      + we call this filtered dataset the "candidate set"  
  
  3. then for each row of the candidate set (each height-cluster pair), we filter the time point 2 dataset to show all isolates found in the selected cluster at the selected height - "candidate isolates"  
      + if any of these isolates were present in the time point 1 dataset, we return isolate_k-height-cluster as a success  
      + if none of the isolates were present in the time point 1 dataset, we move onto the next height-cluster pair in the candidate set  


Column {.tabset}
-------------------------------------
   
### Candidate clusters - novel absorption

Clusters that contain novel genomes (at time point 2).
```{r}
clusters <- timepoint2 %>% filter(isolate %in% isolates_t2)
```

The result of the following is a table of
  height | cluster | type of genome | numbers (of novel, original) | cluster size
using timepoint 2 data

i.e. these are the clusters at all heights that have absorbed novel genomes into clusters
  candidate clusters, with the following requirements:
    - contain novel genomes
    - contain one or more original genomes

```{r}
candidate_clusters <- colnames(timepoint2)[-1] %>% 
  lapply(., function(h_x) {
    h <- as.name(h_x)
  
    # at height h, the clusters that contain novel genomes
    h_cl <- pull(clusters, h) %>% unique()
    dfx <- timepoint2 %>% dplyr::select(isolate, all_of(h)) %>% 
      filter(!!h %in% h_cl)
    dfx$novel <- "original"
    dfx$novel[dfx$isolate %in% isolates_t2] <- "novel"
  
    # sizes of these clusters
    a <- dfx %>% group_by(!!h) %>% tally() %>% set_colnames(c("clusters","cl_size"))
  
    # number of novel genomes and original genomes in each cluster
    b <- dfx %>% group_by(!!h, novel) %>% tally()
    
    # clusters with novel and original genomes
    multistrain_cl <- b %>% filter(novel == "original") %>% pull(h)
    b %<>% filter(!!h %in% multistrain_cl) %>% set_colnames(c("clusters", "type", "count"))
    
    left_join(b, a, by = "clusters") %>% 
      bind_cols(height = as.character(h), .) %>% return()
}) %>% bind_rows()
```

Now, we need to find the first height-cluster pair for each novel genome that first absorbs it.
Next step:
  Find the first cluster-height pair for each novel genome that includes non-novel genomes as well

```{r}
# we run this over all novel genomes
notable_clusters <- isolates_t2 %>% 
  lapply(., function(isolate_x) {
    isolate_x_df <- timepoint2 %>% filter(isolate == isolate_x) %>% 
      dplyr::select(-isolate) %>% t() %>% as.data.frame() %>% 
      rownames_to_column() %>% as_tibble() %>% 
      set_colnames(c("height","clusters"))
    
    absorbing_cluster <- left_join(isolate_x_df, candidate_clusters, 
                                   by = c("height", "clusters")) %>% 
      bind_cols(isolate = isolate_x)
  
    # the first cluster-height pair for isolate_x that includes non-novel genomes
    absorbing_cluster[complete.cases(absorbing_cluster),][1:2,] %>% return()
}) %>% bind_rows()

h_num <- notable_clusters$height %>% gsub("h_", "", .) %>% as.numeric()
notable_clusters %<>% add_column(., h_num, .before = 1)

# note that for the notable_clusters dataset, some height-cluster pairs will be repeated, 
# as they absorb multiple novel genomes at the same time
# also, the number of rows should be double that of the number of novel genomes (a novel and
# original row for each)

# | h_num | height | clusters | type | count | cl_size | isolate |
```

So we have the height-cluster pairs that first absorb the novel genomes, as well as counts of how many novel and original genomes are in each of these clusters, and the cluster sizes. All of this is for height-cluster pairs at timepoint 2.

Next, we rearrange the dataset so we have | isolate | T2_h | T2_cl | T2_cl_size | id (h - cl) |.

```{r}
# | novel isolate | T2_h | T2_cl | T2_cl_size |
novel_absorption <- notable_clusters %>% filter(type == "novel") %>% 
  dplyr::select(-type, -count) %>% unique() %>% 
  dplyr::arrange(h_num, clusters) %>% 
  dplyr::select(isolate, height, clusters, cl_size) %>% 
  set_colnames(c("isolate", "T2_h", "T2_cl", "T2_cl_size"))
novel_absorption$id <- paste(novel_absorption$T2_h, novel_absorption$T2_cl, sep = "-")

novel_absorption %>% 
  set_colnames(c("Novel genome", "T2 height", "T2 cluster", "T2 cluster size", "ID")) %>% 
  DT::datatable(., fillContainer = TRUE, filter = "top", rownames = FALSE,
                options = list(pageLength = nrow(novel_absorption), dom = "ti", scrollY = "500px",
                               columnDefs = list(list(className = "dt-center", targets = 0:4))))
```

Now we have the height-cluster pairs that first absorb each novel genome, as well as the cluster size at time point 2

The first cluster-height pairs for novel isolates that include non-novel genomes (~400), at time point 2.
```{r}
key_clusters_T2 <- novel_absorption %>% 
  dplyr::select(-isolate) %>% unique() %>% 
  dplyr::select(id, T2_h, T2_cl, T2_cl_size)
```

Isolates (and cluster assignments) found at TP2 in key clusters
```{r}
T2_key_assign <- (1:nrow(key_clusters_T2)) %>% 
  lapply(., function(i) {
    row_x <- key_clusters_T2[i,]
    timepoint2 %>% filter(!!as.name(row_x$T2_h) == row_x$T2_cl) %>% 
      dplyr::select(isolate, row_x$T2_h) %>% return()
}) %>% set_names(key_clusters_T2$id)
```

```{r}
key_clusters_T1 <- (1:length(T2_key_assign)) %>% 
  lapply(., function(i) {
    # timepoint 2 isolates in key_cluster i
    iso_in_t2 <- T2_key_assign[[i]] %>% pull(isolate)    # timepoint 2 isolates
    dfx <- timepoint1 %>% filter(isolate %in% iso_in_t2) # T1 cluster assignments
    
    # first T1 height at which all T2 isolates of key cluster are in one T1 cluster
    T1_h <- dfx %>% melt(id = "isolate") %>% as_tibble() %>% 
      group_by(variable, value) %>% tally() %>% 
      filter(n == nrow(dfx)) %>% `[[`(1,1) %>% as.character()
    
    id <- T2_key_assign[i] %>% names() %>% as.character()
    T1_cl <- dfx %>% pull(T1_h) %>% unique()
    
    T1_cl_actual_size <- timepoint1 %>% filter(!!as.name(T1_h) == T1_cl) %>% nrow()
    T1_cl_wo_novel_size <- dfx %>% pull(T1_h) %>% length()
    
    tibble(id, T1_h, T1_cl, T1_cl_wo_novel_size, T1_cl_actual_size) %>% return()
}) %>% bind_rows()
```

Now we have the height-cluster pairs at time point 1 that are associated with the novel-absorbing clusters at time point 2. Note that we use an ID of (T1 height)-(T1 cluster) so we can match the clusters across time points.

```{r}
key_clusters <- full_join(key_clusters_T2, key_clusters_T1, by = "id")

key_clusters$m1 <- key_clusters$T2_cl_size - key_clusters$T1_cl_actual_size
key_clusters$m2 <- key_clusters$T2_cl_size - key_clusters$T1_cl_wo_novel_size

key_clusters$T1_cl %<>% as.character()
key_clusters$T2_cl %<>% as.character()

key_clusters$h1 <- key_clusters$T1_h %>% gsub("h_", "", .) %>% as.integer()
key_clusters$h2 <- key_clusters$T2_h %>% gsub("h_", "", .) %>% as.integer()

key_clusters %<>% dplyr::select(c("id", "h1", "T1_h", "T1_cl", "T1_cl_wo_novel_size", 
                                  "T1_cl_actual_size", "h2", "T2_h", "T2_cl", 
                                  "T2_cl_size", "m1", "m2"))
```

```{r, fig.align='center', fig.width=10}
# note, id comes from the T2 data
kc_plot <- key_clusters %>% 
  dplyr::select(id, h2, T2_cl_size, T1_cl_wo_novel_size, T1_cl_actual_size) %>% 
  set_colnames(c("id", "h2", "Size at T2", "Size without novels", "Actual size at T1")) %>% 
  melt(id = c("id", "h2")) %>% as_tibble() %>% 
  set_colnames(c("ID", "Height", "Variable", "Size"))

{ggplot(kc_plot, aes(x = Height, y = Size, label = ID, colour = Variable)) + 
  geom_point(alpha = 0.6) + 
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) + 
  xlab("Height-cluster pairs") + ylab("Cluster size") + 
  ggtitle("Comparing cluster changes from time points 1 and 2")} %>% 
  ggplotly(tooltip = c("label", "colour", "y"))
```

```{r, fig.align='center', fig.width=10}
ggplot(key_clusters, aes(x = h2, y = m1)) + geom_point() + 
  xlab("Height-cluster pairs") + ylab("Change in cluster size") + 
  ggtitle("Size change for matching (actual) clusters across time points", 
          subtitle = "(In some cases: isolates are found in larger clusters at TP 1 than at TP 2, so negative change)")
```

```{r, fig.align='center', fig.width=10}
ggplot(key_clusters, aes(x = h2, y = m2)) + geom_point() + 
  xlab("Height-cluster pairs") + ylab("Change in cluster size") + 
  ggtitle("Size change for matching conceptual clusters across time points", 
          subtitle = "(This method simply looks at the number of novel genomes added at TP2)")
```

```{r}
dfx <- key_clusters %>% dplyr::select(id, h1, T1_h, T1_cl_wo_novel_size, 
                                      h2, T2_h, T2_cl, T2_cl_size, m2)
# prop is new cluster size over old cluster size
n <- nrow(dfx)
b <- dfx$T2_cl_size[1:n]
a <- dfx$T1_cl_wo_novel_size[1:n]
dfx$growth <- (b - a)/a

growth_thresholds <- seq(0.01, 1, 0.01)
names(growth_thresholds) <- growth_thresholds %>% `*`(100) %>% paste("g", ., sep = "")

param_sweep <- matrix("B", nrow = n, ncol = length(growth_thresholds) + 1) %>% 
  set_colnames(c("id", names(growth_thresholds))) %>% as_tibble()
param_sweep[,1] <- dfx$id

for (i in 1:length(growth_thresholds)) {
  g <- growth_thresholds[i]
  ids <- which(dfx$growth >= g[[1]])
  param_sweep[ids, names(g)] <- "A"
}

param_sweep %<>% left_join(dfx, ., by = "id")
param_sweep %<>% dplyr::select(id, h2, m2, names(growth_thresholds))
```

```{r}
df <- param_sweep %>% melt(id = c("id", "h2", "m2")) %>% 
  set_colnames(c("id", "h2", "m2", "gTh", "c_lbl")) %>% 
  as_tibble()

df_counts <- df
df_counts$c_lbl %<>% as.factor()
df_counts %<>% group_by(gTh) %>% count(c_lbl, .drop = FALSE)

df_percs <- growth_thresholds %>% as.data.frame() %>% rownames_to_column() %>% 
  as_tibble() %>% set_colnames(c("gTh", "perc")) %>% 
  left_join(df_counts, ., by = "gTh")
```

```{r, fig.align='center', fig.width=10}
{ggplot(df_percs, aes(x = perc, y = n, color = c_lbl)) + geom_point() + 
  scale_color_manual(values = c("red", "black"), 
                     labels = c("Grows by >= x", "Grows by < x"), 
                     name = "Height-cluster growth") + 
  scale_x_continuous(labels = scales::percent) + 
  labs(x = "Percent growth", y = "Number of clusters")} %>% ggplotly()
```

```{r}
gbox <- data.frame(x = c(-5, -5, 155, -5),
                   y = c(-10, 65, -10, -10),
                   xend = c(-5, 155, 155, 155),
                   yend = c(65, 65, 65, -10))
p1 <- ggplot(df, aes(h2, m2, color = c_lbl)) + 
  geom_point(alpha = 0.6) + 
  scale_color_manual(name = "Cluster type", values = c("red", "black")) + 
  xlab("Height-cluster pairs") + ylab("Change in cluster size") +
  geom_segment(aes(x=gbox$x[1], y=gbox$y[1],
                   xend=gbox$xend[1], yend=gbox$yend[1]), colour = "black") +
  geom_segment(aes(x=gbox$x[2], y=gbox$y[2],
                   xend=gbox$xend[2], yend=gbox$yend[2]), colour = "black") +
  geom_segment(aes(x=gbox$x[3], y=gbox$y[3],
                   xend=gbox$xend[3], yend=gbox$yend[3]), colour = "black") +
  geom_segment(aes(x=gbox$x[4], y=gbox$y[4],
                   xend=gbox$xend[3], yend=gbox$yend[4]), colour = "black") +
  transition_states(gTh, state_length = 20) +
  labs(title = "Min threshold: {closest_state}")

df2 <- df %>% filter(h2 <= 150)
p2 <- ggplot(df2, aes_string(x = "h2", y = "m2", color = "c_lbl")) + 
  geom_point(alpha = 0.6) + 
  scale_color_manual(name = "Cluster type", values = c("red", "black")) + 
  xlab("Height-cluster pairs") + ylab("Change in cluster size") + 
  ggtitle("Close up of first 100 heights") 
  # geom_segment(aes(x = -5, y = -10, xend = -5, yend = 65), colour = "black") +
  # geom_segment(aes(x = -5, y = 65, xend = 155, yend = 65), colour = "black") +
  # geom_segment(aes(x = 155, y = -10, xend = 155, yend = 65), colour = "black") +
  # geom_segment(aes(x = -5, y = -10, xend = 155, yend = -10), colour = "black") + 
  # transition_states(gTh, state_length = 20) +
  # labs(title = "Min threshold: {closest_state}")
```

```{r}
p1
```

```{r}
p2
```

```{r}
# first column gives the height, the second the position/index where the maximum is reached, the third and forth the indices of where the peak begins and ends --- in the sense of where the pattern starts and ends.

# key_clusters$prop <- key_clusters$T2_cl_size / key_clusters$T1_cl_wo_novel_size
# 
# peaks <- findpeaks(key_clusters$m2) %>%
#   set_colnames(c("Height", "Index", "Begins_index",
#                  "Ends_index")) %>% as_tibble()
# 
# key_clusters$Label <- "B"
# # inds <- intersect(peaks$Index, which(key_clusters$prop >= mean(key_clusters$prop)))
# 
# inds <- intersect(
#   which(key_clusters$m2 >= 15), which(key_clusters$prop >= quantile(key_clusters$prop)[["25%"]])
# )
# # inds <- intersect(peaks$Index, which(key_clusters$m2 >= 10))
# # inds <- which((key_clusters$prop >= median(key_clusters$prop)) & key_clusters$T1_cl_wo_novel_size > 5)
# key_clusters$Label[inds] <- "A"

# key_clusters %>% 
#   ggplot(., aes(x = h2, y = m2, color = Label)) + geom_point(alpha = 0.6) + 

#   xlab("Height-cluster pairs") + ylab("Change in cluster size") + 
#   ggtitle("Size change for matching conceptual clusters across time points", 
#           subtitle = "Notable points are clusters with a change of >= 15 genomes that grow by >= 25%")
```

```{r}
all_heights<- key_clusters %>% 
  dplyr::select(h1, T1_h, T1_cl, T1_cl_wo_novel_size, m2) %>%
  unique() %>% dplyr::arrange(h1, T1_cl)

all_heights$T1_h <- factor(all_heights$T1_h, levels = all_heights$T1_h[order(all_heights$h1)] %>% unique())

q <- ggplot(all_heights, aes(x = T1_h, y = m2)) + geom_boxplot()
q_data <- ggplot_build(q) %>% extract2("data")
q_points <- tibble(Height_num = all_heights$h1 %>% unique(),
                   Height_char = all_heights$T1_h %>% unique(),
                   median = q_data[[1]]$middle,
                   q3 = q_data[[1]]$upper) %>%
  melt(., id.vars = c("Height_num", "Height_char")) %>% as_tibble()
```

```{r, fig.align='center', fig.width=10}
snapshot1 <- all_heights %>% filter(h1 <= 200)
p_points <- q_points %>% filter(Height_num <= 200)
ggplot(snapshot1, aes(x = T1_h, y = m2)) + 
  geom_boxplot() +
  geom_point(data = p_points, aes(x = Height_char, y = value, color = variable, alpha = 0.7)) +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) +
  scale_color_manual(name = "Point color", values = c("median"="red","q3"="blue"),
                       labels = c("Median", "Quantile 3")) +
  xlab(paste0("Heights <= 100")) + ylab("Total cluster size change (TP2 - TP1)") +
  ggtitle("A snapshot of cluster size change at the first heights where novel genomes are absorbed")
```

```{r, fig.align='center', fig.width=10}
# part1 <- new_set %>% filter(Height_num <= 100)
# ggplot(part1, aes(x = Height_char, y = Cluster_size, fill = dataset)) + 
#   geom_boxplot() + xlab("Heights <= 100") + ylab("Cluster sizes") + 
#   scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
```

## Changes in cluster size when genomes are added (at notable heights <= 150). I.e. **Results!**

```{r}
notable_change <- all_heights %>% filter(h1 <= 150) %>%
  dplyr::arrange(-m2) %>%
  dplyr::select(T1_h, T1_cl, m2) %>%
  set_colnames(c("Threshold", "Cluster", "Size change"))

notable_change %>%
  DT::datatable(., fillContainer = TRUE, filter = "top", rownames = TRUE,
                options = list(pageLength = nrow(notable_change), dom = "ti", scrollY = "500px",
                               columnDefs = list(list(className = "dt-center", targets = 0:3))))
```

### Identifying distribution
    
We can see a division into two groups (notable and general), and so a nonlinear SVM might be applicable.

```{r}
# dfx <- key_clusters %>% filter(h2 <= 275)
# x <- dfx$T1_cl_wo_novel_size
# y <- dfx$dif %>% gsub("general", -1, .) %>% gsub("notable", 1, .) %>% as.integer()
# saveRDS(key_clusters, "key_clusters.Rds")
# saveRDS(dfx, "dfx.Rds")

# dat = data.frame(x, y = as.factor(y))
# svmfit = svm(y ~ x, data = dat, kernel = "linear", cost = 10, scale = FALSE)
# print(svmfit)

# make.grid = function(x, n = 75) {
#   grange = apply(x, 2, range)
#   x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
#   x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
#   expand.grid(X1 = x1, X2 = x2)
# }

# xgrid = make.grid(x)
# xgrid[1:10,]
```

## Identifying the distribution of the cluster size changes

```{r}
x <- key_clusters$m2 %>% sort()
# hist(x)
descdist(x)
```

## General method for datasets: a more manual approach for “traditional” distributions {.tabset}

### Normal
```{r}
fit_norm <- fitdist(x, "norm", method = "mge")
plot(fit_norm)
```

### Weibull
```{r}
fit_weibull <- fitdist(x,  distr = "weibull", method = "mge")
plot(fit_weibull)
```

### Exponential
```{r}
fit_exp <- fitdist(x, "exp", method = "mge")
plot(fit_exp)
```

### Gamma
```{r}
fit_gamma <- fitdist(x, "gamma", method = "mge")
plot(fit_gamma)
```

### Lognormal
```{r}
fit_lnorm <- fitdist(x, "lnorm", method = "mge")
plot(fit_lnorm)
```

### Uniform
```{r}
fit_unif <- fitdist(x, "unif", method = "mge")
plot(fit_unif)
```

## Results of distribution identification

```{r}
fitted_models <- list(fit_norm, fit_weibull, fit_exp, 
                      fit_gamma, fit_lnorm, fit_unif) %>% 
  set_names(c("Normal", "Weibull", "Exponential",
              "Gamma", "Lognormal", "Uniform"))

aics <- c(fit_norm$aic, fit_weibull$aic, fit_exp$aic, 
          fit_gamma$aic, fit_lnorm$aic, fit_unif$aic) %>% 
  set_names(c("Normal", "Weibull", "Exponential",
              "Gamma", "Lognormal", "Uniform"))

best_fit_name <- which.min(aics) %>% names()
as.data.frame(aics) %>% rownames_to_column() %>% as_tibble() %>% 
  set_colnames(c("Distribution", "AIC")) %>% kable()
```

The `r tolower(best_fit_name)` has the lowest AIC, so that distribution might be an adequate fit. Now we try a goodness of fit test to verify.  

```{r}
gofTest(x, test = "cvm", distribution = "lnorm")
```

### Identifying distribution

L2 Regularized Linear Support Vector Machines with Class Weights

```{r}
saveRDS(key_clusters, "key_clusters.Rds")
df_ml <- key_clusters %>% filter(h2 <= 275) %>% dplyr::select(h1, T1_cl_wo_novel_size, m2, Label, prop)

# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv",
                              number = 10, repeats = 3)
# RVM with linear kernel
# Train the model
model_lin <- train(Label ~., data = df_ml, method = "svmLinearWeights2",
               trControl = train.control)

# Summarize the results
print(model_lin)
```
















